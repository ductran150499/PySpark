{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "VPWKyG4zUC3n",
        "outputId": "488b8c3c-fefb-4b6a-8a1f-b318656d3e8d"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Collecting pyspark==3.0.1\n",
            "  Downloading pyspark-3.0.1.tar.gz (204.2 MB)\n",
            "\u001b[K     |████████████████████████████████| 204.2 MB 34 kB/s \n",
            "\u001b[?25hCollecting py4j==0.10.9\n",
            "  Downloading py4j-0.10.9-py2.py3-none-any.whl (198 kB)\n",
            "\u001b[K     |████████████████████████████████| 198 kB 40.7 MB/s \n",
            "\u001b[?25hBuilding wheels for collected packages: pyspark\n",
            "  Building wheel for pyspark (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for pyspark: filename=pyspark-3.0.1-py2.py3-none-any.whl size=204612243 sha256=e8dcae3322d62f412104771cb2141055c959f208340e7b8d70859828801f47e3\n",
            "  Stored in directory: /root/.cache/pip/wheels/5e/34/fa/b37b5cef503fc5148b478b2495043ba61b079120b7ff379f9b\n",
            "Successfully built pyspark\n",
            "Installing collected packages: py4j, pyspark\n",
            "Successfully installed py4j-0.10.9 pyspark-3.0.1\n"
          ]
        }
      ],
      "source": [
        "!pip install pyspark==3.0.1 py4j==0.10.9 "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lHGijaFQtlMs",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "55829cd5-b9d8-4a19-e0a3-3d4e425fb7ff"
      },
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**SparkSession**\n",
        "\n",
        "SparkSession has become an entry point to PySpark since version 2.0 earlier the SparkContext is used as an entry point. The SparkSession is an entry point to underlying PySpark functionality to programmatically create PySpark RDD, DataFrame, and Dataset. It can be used in replace with SQLContext, HiveContext, and other contexts defined before 2.0. You should also know that SparkSession internally creates SparkConfig and SparkContext with the configuration provided with SparkSession. SparkSession can be created using SparkSession.builder builder patterns.\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "RlE47HdkUstL"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Creating SparkSession**\n",
        "\n",
        "To create a SparkSession, you need to use the builder pattern method builder()\n"
      ],
      "metadata": {
        "id": "clcZdn1SVnWl"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from pyspark.sql import SparkSession\n",
        "\n",
        "spark = SparkSession.builder\\\n",
        "        .master(\"local[*]\")\\\n",
        "        .appName('PySpark_For_Beginers')\\\n",
        "        .getOrCreate()\n",
        "\n",
        "# where the '*' represents all the cores of the CPU."
      ],
      "metadata": {
        "id": "hrOtpdqAV3gN"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Reading Data and Structuring Data Using Spark Schema**\n",
        "\n",
        "The pyspark can read data from various file formats such as Comma Separated Values (CSV), JavaScript Object Notation (JSON), Parquet, e.t.c. To read different file formats we use spark.read."
      ],
      "metadata": {
        "id": "XJ-o2nbtYb9X"
      }
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5gZkgYK9MjN9"
      },
      "source": [
        "path = \"/content/drive/My Drive/PySpark/\""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Reading the U.S Stock Price data from January 2019 to July 2020 which is available in Kaggle datasets.\n",
        "\n",
        "data = spark.read.csv(path + 'stocks_price_final.csv',\n",
        "                      sep =',',\n",
        "                      header = True,\n",
        "                      )\n",
        "# Printing the schema of the data using PrintSchema method\n",
        "\n",
        "data.printSchema()\n"
      ],
      "metadata": {
        "id": "3DtQOqBVYsMF",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "526ac313-ae93-4c7a-d9b4-c341f342b4c9"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "root\n",
            " |-- _c0: string (nullable = true)\n",
            " |-- symbol: string (nullable = true)\n",
            " |-- date: string (nullable = true)\n",
            " |-- open: string (nullable = true)\n",
            " |-- high: string (nullable = true)\n",
            " |-- low: string (nullable = true)\n",
            " |-- close: string (nullable = true)\n",
            " |-- volume: string (nullable = true)\n",
            " |-- adjusted: string (nullable = true)\n",
            " |-- market.cap: string (nullable = true)\n",
            " |-- sector: string (nullable = true)\n",
            " |-- industry: string (nullable = true)\n",
            " |-- exchange: string (nullable = true)\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Spark schema** is the structure of the DataFrame or Dataset, we can define it using **StructType** class which is a collection of **StructField** that defines the column name(String), column type (DataType), nullable column (Boolean), and metadata (MetaData). spark infers the schema from data however some times the inferred datatype may not be correct or we may need to define our own column names and data types, especially while working with unstructured and semi-structured data."
      ],
      "metadata": {
        "id": "VgOJsQIycvDw"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from pyspark.sql.types import *\n",
        "\n",
        "# Creating structure using StructType and StructField \n",
        "\n",
        "data_schema = [\n",
        "               StructField('_c0', IntegerType(), True),\n",
        "               StructField('symbol', StringType(), True),\n",
        "               StructField('data', DateType(), True),\n",
        "               StructField('open', DoubleType(), True),\n",
        "               StructField('high', DoubleType(), True),\n",
        "               StructField('low', DoubleType(), True),\n",
        "               StructField('close', DoubleType(), True),\n",
        "               StructField('volume', IntegerType(), True),\n",
        "               StructField('adjusted', DoubleType(), True),\n",
        "               StructField('market.cap', StringType(), True),\n",
        "               StructField('sector', StringType(), True),\n",
        "               StructField('industry', StringType(), True),\n",
        "               StructField('exchange', StringType(), True),\n",
        "            ]\n",
        "\n",
        "final_struc = StructType(fields = data_schema)\n",
        "\n",
        "# Reading the data using spark.read.csv() and see the schema of the structured data at final\n",
        "\n",
        "data = spark.read.csv(\n",
        "    path + 'stocks_price_final.csv',\n",
        "    sep = ',',\n",
        "    header = True,\n",
        "    schema = final_struc \n",
        "    )\n",
        "\n",
        "data.printSchema()"
      ],
      "metadata": {
        "id": "JIzbIVLXc2YN",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "0bb9b2cb-06f8-40df-a06c-a9942814d4a0"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "root\n",
            " |-- _c0: integer (nullable = true)\n",
            " |-- symbol: string (nullable = true)\n",
            " |-- data: date (nullable = true)\n",
            " |-- open: double (nullable = true)\n",
            " |-- high: double (nullable = true)\n",
            " |-- low: double (nullable = true)\n",
            " |-- close: double (nullable = true)\n",
            " |-- volume: integer (nullable = true)\n",
            " |-- adjusted: double (nullable = true)\n",
            " |-- market.cap: string (nullable = true)\n",
            " |-- sector: string (nullable = true)\n",
            " |-- industry: string (nullable = true)\n",
            " |-- exchange: string (nullable = true)\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Different Methods to Inspect Data**"
      ],
      "metadata": {
        "id": "r0zs8gewfzjf"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# schema(): returns the schema of the data(dataframe)\n",
        "\n",
        "data.printSchema()"
      ],
      "metadata": {
        "id": "pUoznlcDf4XA",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "0881bd5a-7685-439f-ebb5-1934320d0614"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "root\n",
            " |-- _c0: integer (nullable = true)\n",
            " |-- symbol: string (nullable = true)\n",
            " |-- data: date (nullable = true)\n",
            " |-- open: double (nullable = true)\n",
            " |-- high: double (nullable = true)\n",
            " |-- low: double (nullable = true)\n",
            " |-- close: double (nullable = true)\n",
            " |-- volume: integer (nullable = true)\n",
            " |-- adjusted: double (nullable = true)\n",
            " |-- market.cap: string (nullable = true)\n",
            " |-- sector: string (nullable = true)\n",
            " |-- industry: string (nullable = true)\n",
            " |-- exchange: string (nullable = true)\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# dtypes: returns a list of tuples with the columns names and it's data types\n",
        "\n",
        "data.dtypes"
      ],
      "metadata": {
        "id": "Witq4g_Eg99x",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "31cd5499-cb81-4b41-ccb3-68a212481fdc"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[('_c0', 'int'),\n",
              " ('symbol', 'string'),\n",
              " ('data', 'date'),\n",
              " ('open', 'double'),\n",
              " ('high', 'double'),\n",
              " ('low', 'double'),\n",
              " ('close', 'double'),\n",
              " ('volume', 'int'),\n",
              " ('adjusted', 'double'),\n",
              " ('market.cap', 'string'),\n",
              " ('sector', 'string'),\n",
              " ('industry', 'string'),\n",
              " ('exchange', 'string')]"
            ]
          },
          "metadata": {},
          "execution_count": 17
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# head(n): returns n rows as a list\n",
        "\n",
        "data.head(5)"
      ],
      "metadata": {
        "id": "dlcRjfiihZbU",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "ef5d6442-b884-46cb-91d1-0209308a4c78"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[Row(_c0=1, symbol='TXG', data=datetime.date(2019, 9, 12), open=54.0, high=58.0, low=51.0, close=52.75, volume=7326300, adjusted=52.75, market.cap='$9.31B', sector='Capital Goods', industry='Biotechnology: Laboratory Analytical Instruments', exchange='NASDAQ'),\n",
              " Row(_c0=2, symbol='TXG', data=datetime.date(2019, 9, 13), open=52.75, high=54.355, low=49.150002, close=52.27, volume=1025200, adjusted=52.27, market.cap='$9.31B', sector='Capital Goods', industry='Biotechnology: Laboratory Analytical Instruments', exchange='NASDAQ'),\n",
              " Row(_c0=3, symbol='TXG', data=datetime.date(2019, 9, 16), open=52.450001, high=56.0, low=52.009998, close=55.200001, volume=269900, adjusted=55.200001, market.cap='$9.31B', sector='Capital Goods', industry='Biotechnology: Laboratory Analytical Instruments', exchange='NASDAQ'),\n",
              " Row(_c0=4, symbol='TXG', data=datetime.date(2019, 9, 17), open=56.209999, high=60.900002, low=55.423, close=56.779999, volume=602800, adjusted=56.779999, market.cap='$9.31B', sector='Capital Goods', industry='Biotechnology: Laboratory Analytical Instruments', exchange='NASDAQ'),\n",
              " Row(_c0=5, symbol='TXG', data=datetime.date(2019, 9, 18), open=56.849998, high=62.27, low=55.650002, close=62.0, volume=1589600, adjusted=62.0, market.cap='$9.31B', sector='Capital Goods', industry='Biotechnology: Laboratory Analytical Instruments', exchange='NASDAQ')]"
            ]
          },
          "metadata": {},
          "execution_count": 18
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# show(): displays the first 20 rows by default and it also takes a number as a parameter to display the number of rows of the data\n",
        "\n",
        "data.show()"
      ],
      "metadata": {
        "id": "s4021jo3h012",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "f76403f7-5794-43d4-ee68-1afc8871b3ea"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+---+------+----------+---------+---------+---------+---------+-------+---------+----------+-------------+--------------------+--------+\n",
            "|_c0|symbol|      data|     open|     high|      low|    close| volume| adjusted|market.cap|       sector|            industry|exchange|\n",
            "+---+------+----------+---------+---------+---------+---------+-------+---------+----------+-------------+--------------------+--------+\n",
            "|  1|   TXG|2019-09-12|     54.0|     58.0|     51.0|    52.75|7326300|    52.75|    $9.31B|Capital Goods|Biotechnology: La...|  NASDAQ|\n",
            "|  2|   TXG|2019-09-13|    52.75|   54.355|49.150002|    52.27|1025200|    52.27|    $9.31B|Capital Goods|Biotechnology: La...|  NASDAQ|\n",
            "|  3|   TXG|2019-09-16|52.450001|     56.0|52.009998|55.200001| 269900|55.200001|    $9.31B|Capital Goods|Biotechnology: La...|  NASDAQ|\n",
            "|  4|   TXG|2019-09-17|56.209999|60.900002|   55.423|56.779999| 602800|56.779999|    $9.31B|Capital Goods|Biotechnology: La...|  NASDAQ|\n",
            "|  5|   TXG|2019-09-18|56.849998|    62.27|55.650002|     62.0|1589600|     62.0|    $9.31B|Capital Goods|Biotechnology: La...|  NASDAQ|\n",
            "|  6|   TXG|2019-09-19|62.810001|   63.375|61.029999|61.119999| 425200|61.119999|    $9.31B|Capital Goods|Biotechnology: La...|  NASDAQ|\n",
            "|  7|   TXG|2019-09-20|61.709999|62.419998|    59.82|     60.5| 392000|     60.5|    $9.31B|Capital Goods|Biotechnology: La...|  NASDAQ|\n",
            "|  8|   TXG|2019-09-23|60.220001|61.485001|59.939999|60.330002| 137200|60.330002|    $9.31B|Capital Goods|Biotechnology: La...|  NASDAQ|\n",
            "|  9|   TXG|2019-09-24|     61.0|     61.0|     54.0|54.299999| 713800|54.299999|    $9.31B|Capital Goods|Biotechnology: La...|  NASDAQ|\n",
            "| 10|   TXG|2019-09-25|54.459999|55.880001|   52.563|52.759998| 261200|52.759998|    $9.31B|Capital Goods|Biotechnology: La...|  NASDAQ|\n",
            "| 11|   TXG|2019-09-26|52.779999|53.689999|46.619999|49.990002| 596300|49.990002|    $9.31B|Capital Goods|Biotechnology: La...|  NASDAQ|\n",
            "| 12|   TXG|2019-09-27|51.130001|     55.0|50.700001|51.029999| 621300|51.029999|    $9.31B|Capital Goods|Biotechnology: La...|  NASDAQ|\n",
            "| 13|   TXG|2019-09-30|51.049999|     52.0|    49.25|50.400002| 168900|50.400002|    $9.31B|Capital Goods|Biotechnology: La...|  NASDAQ|\n",
            "| 14|   TXG|2019-10-01|50.509998|51.919998|     46.0|47.029999| 536300|47.029999|    $9.31B|Capital Goods|Biotechnology: La...|  NASDAQ|\n",
            "| 15|   TXG|2019-10-02|46.779999|    47.23|45.110001|    46.07| 519600|    46.07|    $9.31B|Capital Goods|Biotechnology: La...|  NASDAQ|\n",
            "| 16|   TXG|2019-10-03|    46.77|48.240002|    45.75|48.119999| 703900|48.119999|    $9.31B|Capital Goods|Biotechnology: La...|  NASDAQ|\n",
            "| 17|   TXG|2019-10-04|     48.0|    53.34|    47.82|51.450001| 322400|51.450001|    $9.31B|Capital Goods|Biotechnology: La...|  NASDAQ|\n",
            "| 18|   TXG|2019-10-07|52.099998|53.220001|49.029999|50.360001| 476600|50.360001|    $9.31B|Capital Goods|Biotechnology: La...|  NASDAQ|\n",
            "| 19|   TXG|2019-10-08|     50.0|    51.27|     49.0|49.549999| 284100|49.549999|    $9.31B|Capital Goods|Biotechnology: La...|  NASDAQ|\n",
            "| 20|   TXG|2019-10-09|49.630001|51.525002|49.575001|50.009998| 201100|50.009998|    $9.31B|Capital Goods|Biotechnology: La...|  NASDAQ|\n",
            "+---+------+----------+---------+---------+---------+---------+-------+---------+----------+-------------+--------------------+--------+\n",
            "only showing top 20 rows\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# first(): returns the first row of the data\n",
        "\n",
        "data.first()"
      ],
      "metadata": {
        "id": "bxdzdgjqjk_G",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "c89cc339-77d8-44f8-eae4-88e979155a94"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "Row(_c0=1, symbol='TXG', data=datetime.date(2019, 9, 12), open=54.0, high=58.0, low=51.0, close=52.75, volume=7326300, adjusted=52.75, market.cap='$9.31B', sector='Capital Goods', industry='Biotechnology: Laboratory Analytical Instruments', exchange='NASDAQ')"
            ]
          },
          "metadata": {},
          "execution_count": 20
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# take(n): returns the first n rows of the data\n",
        "\n",
        "data.take(5)"
      ],
      "metadata": {
        "id": "r0lqjFJwkvwV",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "78f43e0d-a9b3-4735-ff89-c3a51fa61cfe"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[Row(_c0=1, symbol='TXG', data=datetime.date(2019, 9, 12), open=54.0, high=58.0, low=51.0, close=52.75, volume=7326300, adjusted=52.75, market.cap='$9.31B', sector='Capital Goods', industry='Biotechnology: Laboratory Analytical Instruments', exchange='NASDAQ'),\n",
              " Row(_c0=2, symbol='TXG', data=datetime.date(2019, 9, 13), open=52.75, high=54.355, low=49.150002, close=52.27, volume=1025200, adjusted=52.27, market.cap='$9.31B', sector='Capital Goods', industry='Biotechnology: Laboratory Analytical Instruments', exchange='NASDAQ'),\n",
              " Row(_c0=3, symbol='TXG', data=datetime.date(2019, 9, 16), open=52.450001, high=56.0, low=52.009998, close=55.200001, volume=269900, adjusted=55.200001, market.cap='$9.31B', sector='Capital Goods', industry='Biotechnology: Laboratory Analytical Instruments', exchange='NASDAQ'),\n",
              " Row(_c0=4, symbol='TXG', data=datetime.date(2019, 9, 17), open=56.209999, high=60.900002, low=55.423, close=56.779999, volume=602800, adjusted=56.779999, market.cap='$9.31B', sector='Capital Goods', industry='Biotechnology: Laboratory Analytical Instruments', exchange='NASDAQ'),\n",
              " Row(_c0=5, symbol='TXG', data=datetime.date(2019, 9, 18), open=56.849998, high=62.27, low=55.650002, close=62.0, volume=1589600, adjusted=62.0, market.cap='$9.31B', sector='Capital Goods', industry='Biotechnology: Laboratory Analytical Instruments', exchange='NASDAQ')]"
            ]
          },
          "metadata": {},
          "execution_count": 21
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# describe(): computes the summary statistics of the columns with the numeric data type\n",
        "\n",
        "data.describe().show()"
      ],
      "metadata": {
        "id": "rKdezYgTlFA2",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "eaafb623-df49-42d5-ce68-ea586c2a5ecd"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+-------+-----------------+-------+------------------+------------------+------------------+------------------+------------------+------------------+----------+----------------+--------------------+--------+\n",
            "|summary|              _c0| symbol|              open|              high|               low|             close|            volume|          adjusted|market.cap|          sector|            industry|exchange|\n",
            "+-------+-----------------+-------+------------------+------------------+------------------+------------------+------------------+------------------+----------+----------------+--------------------+--------+\n",
            "|  count|          1729034|1729034|           1726301|           1726301|           1726301|           1726301|           1725207|           1726301|   1729034|         1729034|             1729034| 1729034|\n",
            "|   mean|         864517.5|   null|15070.071703341051| 15555.06726813709|14557.808227578982|15032.714854330707|1397692.1627885813|  14926.1096887955|      null|            null|                null|    null|\n",
            "| stddev|499129.2670065541|   null|1111821.8002863196|1148247.1953514954|1072968.1558434265|1109755.9294000647| 5187522.908169119|1101877.6328940107|      null|            null|                null|    null|\n",
            "|    min|                1|      A|             0.072|             0.078|             0.052|             0.071|                 0|         -1.230099|    $1.01B|Basic Industries|Accident &Health ...|  NASDAQ|\n",
            "|    max|          1729034|   ZYXI|      1.60168176E8|      1.61601456E8|      1.55151728E8|      1.58376592E8|         656504200|      1.57249392E8|       $9B|  Transportation|Wholesale Distrib...|    NYSE|\n",
            "+-------+-----------------+-------+------------------+------------------+------------------+------------------+------------------+------------------+----------+----------------+--------------------+--------+\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "uGxZ7i_clS39"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# columns: returns a list that contains the column names of the data\n",
        "\n",
        "data.columns"
      ],
      "metadata": {
        "id": "7MYU5Hsll6i2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# count(): returns the count of the number of rows in the data\n",
        "\n",
        "data.count()"
      ],
      "metadata": {
        "id": "K49KttYemJv-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# distinct(): returns the number of the distict rows in the data\n",
        "\n",
        "data.distinct()"
      ],
      "metadata": {
        "id": "y3NjwykxmTAV"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Columns Manipulation**\n",
        "\n",
        "There are different methods that are used to add, update, delete, columns of the data."
      ],
      "metadata": {
        "id": "CgGVwzBcmzT4"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Adding Column: Use withColumn the method takes two parameters column name and data to add a new column to the existing data\n",
        "\n",
        "data = data.withColumn('date', data.data)\n",
        "data.show()"
      ],
      "metadata": {
        "id": "q83ctHYQn93d"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Update column: Use withColumnRenamed which takes two parameters existing column name and new column name to rename the existing column.\n",
        "\n",
        "data = data.withColumnRenamed('date', 'data_changed')\n",
        "data.show()"
      ],
      "metadata": {
        "id": "tAGDaqr-oFd7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Delete Column: Use drop the method which takes the column name and returns the data.\n",
        "\n",
        "data = data.drop('data_changed')\n",
        "data.show()"
      ],
      "metadata": {
        "id": "nyLPpDSRqcnm"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Dealing with Missing Values**\n",
        "\n",
        "We often encounter missing values while dealing with real-time data. These missing values are encoded as NaN, Blanks, and placeholders. There are various techniques to deal with missing values some of the popular ones are:\n",
        "\n",
        "**Remove**: Remove the rows having missing values in any one of the columns.\n",
        "\n",
        "**Impute with Mean/Median**: Replace the missing values using the Mean/Median of the respective column. It’s easy, fast, and works well with small numeric datasets.\n",
        "\n",
        "**Impute with Most Frequent Values**: As the name suggests use the most frequent value in the column to replace the missing value of that column. This works well with categorical features and may also introduce bias into the data.\n",
        "\n",
        "**Impute using KNN**: K-Nearest Neighbors is a classification algorithm that uses feature similarity using different distance metrics such as Euclidean, Mahalanobis, Manhattan, Minkowski, and Hamming e.t.c. for any new data points. This is very efficient compared to the above-mentioned methods to impute missing values depending on the dataset and it is computationally expensive and sensitive to outliers."
      ],
      "metadata": {
        "id": "D2-NwoS-q0rX"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Using the Dataset: https://www.kaggle.com/datasets/dansbecker/melbourne-housing-snapshot?select=melb_data.csv\n",
        "\n",
        "data1 = spark.read.csv('/var/melb_data.csv')\n",
        "data1.show()"
      ],
      "metadata": {
        "id": "BjWhbaV7rGLH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Remove rows with mising values\n",
        "\n",
        "data1.na.drop().show()"
      ],
      "metadata": {
        "id": "PY6z-C622d4I"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Filling Missing Values with NA.fill()\n",
        "\n",
        "data1.na.fill('CMC Global', '_c14').show()"
      ],
      "metadata": {
        "id": "Fs9qlgp2O20Y"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Imputing NA values with central tendency measured**\n",
        "\n",
        "This is something of a more professional way to handle the missing values i.e imputing the null values with mean/median/mode depending on the domain of the dataset. Here we will be using the Imputer function from the PySpark library to use the mean/median/mode functionality."
      ],
      "metadata": {
        "id": "47fkeGAmdbRB"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from pyspark.ml.feature import Imputer\n",
        "\n",
        "imputer = Imputer(\n",
        "    inputCols = ['open'],\n",
        "    outputCols = [\"{}_imputed\".format(a) for a in ['open']]\n",
        ").setStrategy(\"mean\")"
      ],
      "metadata": {
        "id": "1wUK5OxBd_nC"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Fit and Transform**\n",
        "\n",
        "Now so we have used the Imputer object to impute the mean values in the place of null values but to see the changes we need to use the fit-transform method simultaneously.\n"
      ],
      "metadata": {
        "id": "-5UkoE5rfrmk"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "imputer.fit(data).transform(data).show()"
      ],
      "metadata": {
        "id": "u33Q8Io3gLrW"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Querying Data**"
      ],
      "metadata": {
        "id": "SUPq72Rzq0JJ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "The PySpark and PySpark SQL provide a wide range of methods and functions to query the data at ease. Here are the few most used methods:\n",
        "\n",
        "Select  Filter  Between  When  Like  GroupBy  Aggregations\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "O0NtumtErEYq"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# SELECT: It is used to select single or multiples columns using the names of the columns\n",
        "# Selecting single column \n",
        "\n",
        "data.select('sector').show(5)\n",
        "\n",
        "# Selecting Multiple columns\n",
        "\n",
        "data.select(['open', 'close', 'adjusted']).show(5)"
      ],
      "metadata": {
        "id": "h-8I5MGtrFAC"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# FILTER: filter the data based on the given condition, you can also give multiple conditions using AND(&), OR(|), and NOT(~) operators.\n",
        "\n",
        "from pyspark.sql.functions import col, lit\n",
        "\n",
        "data.filter((data.open > 54) & (data.low == 51)).show(5)\n"
      ],
      "metadata": {
        "id": "FuZ_3BZBsF9H"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Data Visualization**\n",
        "\n",
        "We are going to utilize matplotlib and pandas to visualize data, the toPandas() method used to convert the data into pandas dataframe. Using the dataframe we utilize the plot() method to visualize data. The below code shows how to display a bar graph for the average opening, closing, and adjusted stock price concerning the sector.\n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "kVeqNIJf0s-7"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from matplotlib import pyplot as plt"
      ],
      "metadata": {
        "id": "0mC5uiRA0kZX"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "sec_df =  data.select(['sector', \n",
        "                       'open', \n",
        "                       'close', \n",
        "                       'adjusted']\n",
        "                     )\\\n",
        "                     .groupBy('sector')\\\n",
        "                     .mean()\\\n",
        "                     .toPandas()\n",
        "\n",
        "ind = list(range(12))\n",
        "\n",
        "ind.pop(6)\n",
        "\n",
        "sec_df.iloc[ind ,:].plot(kind = 'bar', x='sector', y = sec_df.columns.tolist()[1:], \n",
        "                         figsize=(12, 6), ylabel = 'Stock Price', xlabel = 'Sector')\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "leQZ2Wnd0RN8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "xhqFf_k5XNA5"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}